[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Remington Neal: Cura 2026",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Remington Neal: Cura 2026",
    "section": "Background",
    "text": "Background\nThis book is the result of various machine learning trials using microbiome data collected by Dr. Chaston and Dr. Lundwall of BYU. This is a preliminary analysis and should not be seen as a finalized analysis.\nMachine learning models were trained on fractional abundance of OTUs collected as part of a study involving children with elivated likelihoods have haveing ASD. Models were trained using the fractional abundance to predict various",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#models-used",
    "href": "index.html#models-used",
    "title": "Remington Neal: Cura 2026",
    "section": "Models used",
    "text": "Models used\n\nRandom Forest Classification\nThis is the primary classification tool used. A random forest classifier is an ensamble of binary classification trees. Features are bootstrap sampled and given to classifications trees to predict the class the feature belongs to. After each tree has made it’s prediction, the results are combined to determine the model’s final answer.\nEach tree’s final prediction is weighted so some trees have more say in the final result than others. This allows researchers to analize each trees weight to determine which feature has the most predictive power within the model.\n\n\nPCA\nPrinciple Component Analysis, PCA, uses linear algebra to recuce the number of features used to analyze. PCA is used in these analysis to reduce the number of features and increase the predictive power of the random forest classifier.\n\n\nScaler\nThe scaler restricts the features values to be between -1 and 1. This standardization does not modify the relationship between values but transformes then to help the model increase predictive power.\n\n\nGrid Search\nGrid search is not a machine learning model but a wrapper around models allowing researchers to train multiple models with different hyperparameters to create a model that best fits a given metric (precision, recall, accuracy, etc.).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#evaluations",
    "href": "index.html#evaluations",
    "title": "Remington Neal: Cura 2026",
    "section": "Evaluations",
    "text": "Evaluations\nModels are evaluated on 6 different metrics: 1. Accuracy - the percent of correct predictions 2. Precision - the percent of true positives given all positive predictions 3. Recall - the percent of true positives given all true values 4. F-beta - the harmonic mean between precision and recall 5. McNemar’s test - a chi squared metric created using the false positive and false negative predictions 6. p-value: a p-value created based on the chi squared statistic from McNemar’s test\nAll equations for each evaluation statistic can be found in the supplamentary section.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This Quarto book is part of a Remington Manning-Neal’s BYU Life Sciences CURA project.\nThis project uses machine learning using Python’s SKLearn machine learning package too perform classification analysis on data collected by Chaston Lab in the PWS department and Lundwall Lab in the Psycology department.\nData includes is relative abundance of OTUs found within stool samples from children under the age of 3 with an elivated chance of having autism as well as a control population, and meta data about individuals in these two groups.\nMeta data about ADOs scores is used to identify if a child may be autisic and is used as a classification target for my analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_grid_search.html",
    "href": "asd_concern_random_forest_with_grid_search.html",
    "title": "2  Basic Random-Forest analysis of autism concern groups",
    "section": "",
    "text": "2.1 Preface\nThis document is written using Quarto, a modern scientific documentation tool by Postix. This tool functions like Rmarkdown or Jupyter and actually uses those tools to run the documents. It provides flexability in rendering documents and intigrates well with Posit’s other product, R Studio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism concern groups</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_grid_search.html#imports",
    "href": "asd_concern_random_forest_with_grid_search.html#imports",
    "title": "2  Basic Random-Forest analysis of autism concern groups",
    "section": "2.2 Imports",
    "text": "2.2 Imports\n\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport yaml\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import\\\n     accuracy_score,\\\n     confusion_matrix,\\\n     f1_score, \\\n     recall_score, \\\n     make_scorer, \\\n     precision_recall_fscore_support\nfrom sklearn.model_selection import \\\n     GridSearchCV, \\\n     StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import chi2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism concern groups</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_grid_search.html#get-data",
    "href": "asd_concern_random_forest_with_grid_search.html#get-data",
    "title": "2  Basic Random-Forest analysis of autism concern groups",
    "section": "2.3 Get Data",
    "text": "2.3 Get Data\n\n2.3.1 Load Config\nLoading data into env\n\nwith open('config.yaml', mode='r', encoding=None) as f:\n    config_data = yaml.safe_load(f)\n\nfigure_export = config_data[\"figure_export\"] # default true\ndata_sources = config_data[\"data\"]\nparam_grid = config_data[\"param_grid\"]\nhyperparameters = config_data[\"hyperparameters\"]\n\nYaml config data\n\n\nfigure_export: true\ndata:\n  rarefied_table: \"data/rarefied_table.txt\"\n  mapper: \"data/mapper_book_copy_with_new_meta_titles copy.xlsx\"\nhyperparameters:\n  random_state: 42\n  training_split: 0.2\n  estimators: 100\n  n_jobs: -1\n  n_components: 0.60\n\nparam_grid:\n    max_features:\n      - ~\n      - sqrt\n      - log2\n    criterion: \n      - gini\n      - entropy\n      - log_loss\n    'class_weight':\n      - ~\n      - balanced\n\n\n\n\n\n2.3.2 Hyperparameters\nThese hyperparameters are established in the config.yaml file but can be overriden using command line values using -P variable:value -P othervariable:value syntax.\n\n# above tag allows parameters to be passed via cli\nrandomState = hyperparameters[\"random_state\"]\ntrainingSplit = hyperparameters[\"training_split\"]\nestimators = hyperparameters[\"estimators\"]\nn_jobs = hyperparameters[\"n_jobs\"]\n\n\n\n2.3.3 Load dataframes\n\nrarefiedData = pd.read_csv(data_sources[\"rarefied_table\"], sep=\"\\t\")\n\nmapper = pd.read_excel(data_sources[\"mapper\"], sheet_name=\"Mapper 2025-03-21\")\n\nrarefiedData = rarefiedData.T\nrarefiedData.columns = rarefiedData.iloc[0].values\nrarefiedData = rarefiedData.iloc[1:]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism concern groups</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_grid_search.html#create-training-and-testing-sets",
    "href": "asd_concern_random_forest_with_grid_search.html#create-training-and-testing-sets",
    "title": "2  Basic Random-Forest analysis of autism concern groups",
    "section": "2.4 Create training and testing sets",
    "text": "2.4 Create training and testing sets\n\n2.4.1 Combining rarefied and asd status\n\nadosClassification = mapper.loc[:, [\"IDnumber\", \"@SampleID\", \"ADOSClassification\"]]\n\n# Filters out any potential stray IDs not part of out two groups\nadosClassification = adosClassification[adosClassification[\"IDnumber\"].str.contains('^[777|888]')]\n\nautismClassification = adosClassification\nautismClassification[\"ADOSClassification\"] = \\\n    adosClassification[\"ADOSClassification\"].astype(str)\nautismClassification = \\\n    autismClassification[autismClassification[\"ADOSClassification\"] != \"nan\"]\n\nautismClassification[\"Autism\"] = \\\n    autismClassification[\"ADOSClassification\"].transform(\n        lambda x: 0 if x == \"Minimal to No concern\" else 1\n        )\nautismClassification = autismClassification.drop(\n    columns = [\"ADOSClassification\"]\n    )\n\nadosRarefiedData = pd.merge(\n    autismClassification, \n    rarefiedData, \n    left_on = \"@SampleID\", \n    right_index = True, how=\"inner\")\nadosRarefiedData\n\n/var/folders/0w/00j3lxtn6lq5n0jfhbsvrdd00000gn/T/ipykernel_79813/3272697402.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  autismClassification[\"Autism\"] = \\\n\n\n\n\n\n\n\n\n\nIDnumber\n@SampleID\nAutism\n5fa6f990436cc98852c849cfc171976d\n9fc95dae73ec157e4f5ade5e0b205d51\nbf7ae65173c37bc3b210305f65760434\n523f0825433a02f5156e465f0972bc02\ncf87b63430ff2925721ab988ad584f83\n3c142dbbe6d67603d586ecb204a34cee\nec24451f7df9ad0d4edc00d327ba5b95\n...\na3b79adce4a953d4c1c8777dafc43a64\n86337e85ead9f35be4922027ed204e47\n4bfc0f32efe482acd73c942b8d7cd588\n35f42945e50c96278af7e268f124d56d\nd7656aada2e1e434542c5b7ed5592105\nd7bee6730fc4894b26aee43930e74710\n21d97d92de8f9fda7cc194d3245f039b\n304df219bf9ecbefbec639606e7bb98a\nf75aac2353d9e8da5f200c15cf7a564e\ne8dfa53470afb7592c0a10ecd09f758f\n\n\n\n\n0\n777-001\nasd965\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n777-001\nasd966\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.2\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n777-001\nasd1132\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n777-001\nasd1133\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.8\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n777-001\nasd1534\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n287\n888-029\nasd1407\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n288\n888-029\nasd1134\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n289\n888-029\nasd1135\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n290\n888-029\nasd1498\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n291\n888-029\nasd1499\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.2\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n184 rows × 847 columns\n\n\n\n\n\n2.4.2 Dividing training and testing\nThe following cells divide the data into training and testing sets. In order to keep the data clean, samples collected from a single individual, the IDnumber column, are divided togeather to avoid data bleed.\n\n# filter IDnumber and Autism \n# Y = autism, X = IDnumber\n# filter whole table on IDnumber\n# select real X values\ntemp_data = adosRarefiedData.loc[:,[\"IDnumber\", \"Autism\"]]\ntemp_data = temp_data.drop_duplicates()\nX_temp = temp_data.loc[:, \"IDnumber\"]\ny_temp = temp_data.loc[:, \"Autism\"]\n\nX_train_temp, X_test_temp, y_train_temp, y_test_temp = \\\n    train_test_split(X_temp, y_temp, test_size=trainingSplit,random_state=randomState)\n\n\ntrain_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_train_temp)]\ntest_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_test_temp)]\n\nnot_in_y = [\"Autism\", \"IDnumber\", \"@SampleID\"]\n\ny_train = train_data[\"Autism\"].to_frame()\nX_train = train_data.drop(columns = not_in_y)\ny_test = test_data[\"Autism\"].to_frame()\nX_test = test_data.drop(columns = not_in_y)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism concern groups</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_grid_search.html#training",
    "href": "asd_concern_random_forest_with_grid_search.html#training",
    "title": "2  Basic Random-Forest analysis of autism concern groups",
    "section": "2.5 Training",
    "text": "2.5 Training\nThe scaler rescales the features to fit between -1 and 1. This does not effect the outcome of the random forest but can help standardize inputs to increase efficiancy of training.\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_test = y_test.to_numpy().ravel()\ny_train = y_train.to_numpy().ravel()\n\nGridSearchCV allows to train multiple Random Forest classifiers using various combinations of hyperparameters. The number of models trained is equal to \\((\\prod_{i=0}^{n}{|h_{i}|})*cv\\) where \\(|h_{i}|\\) is the size of the param param_grid list and \\(cv\\) is the cross validation number. StratifiedKFold divides the training data into n “folds”. Each pass of training selects one of these folds to act as a validation set to track the model’s training. More information can be found here. 5 is an arbitrary value.\n\nclassifier = RandomForestClassifier(n_estimators=estimators, random_state=randomState)\ncustom_recall = make_scorer(recall_score, pos_label=1)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=randomState)\n\nsearch = GridSearchCV(\n    estimator=classifier,\n    n_jobs=n_jobs,\n    param_grid=param_grid,\n    cv=cv,\n    scoring=\"recall\")\nsearch.fit(X_train, y_train)\nsearch.best_estimator_.get_params()\n\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'entropy',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': 42,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\n2.5.1 Prediction\n\ny_pred = search.predict(X_test)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism concern groups</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_grid_search.html#evaluations",
    "href": "asd_concern_random_forest_with_grid_search.html#evaluations",
    "title": "2  Basic Random-Forest analysis of autism concern groups",
    "section": "2.6 Evaluations",
    "text": "2.6 Evaluations\nPrecision and recall are inversly preportional. Both values are combined in the F_beta score. F_beta is the harmonic mean of precision and recall scaled using the \\(\\beta\\) value. \\(\\beta\\) determines if precision or recall is weighted more in the final score. \\(\\beta = 1\\) equally balances both metrics; \\(\\beta &gt; 1\\) increases recalls weight and \\(\\beta &lt; 1\\) increases precisions weight. In this case, \\(\\beta\\) is set to 1.\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprecision, recall, fscore, _ = \\\n    precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\n\ntickLabels = [\"Mild-Severe Concern\", \"No Concern\"]\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False,\n            xticklabels=tickLabels, yticklabels=tickLabels)\n\nplt.title('Autism Predictions Confusion Matrix Heatmap')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n# McNemar's test\nchi2_stat = (conf_matrix[0,1] - conf_matrix[1,0])**2 / (conf_matrix[0,1] + conf_matrix[1,0])\np_val = chi2.sf(chi2_stat, df=1)\n\nprint(\n    f\"Accuracy: {accuracy * 100:.2f}%\\n\" \\\n    f\"Precision: {precision* 100:.2f}%\\n\" \\\n    f\"Recall: {recall* 100:.2f}%\\n\" \\\n    f\"F beta: {fscore:.2f}\\n\" \\\n    f\"Chi2 stat (McNemar's test): {chi2_stat:.2f}\\n\" \\\n    f\"Degrees of Freedom: {1}\\n\" \\\n    f\"p-value: {p_val:.2f}\"\n    )\n\n\n\n\n\n\n\n\nAccuracy: 76.32%\nPrecision: 0.00%\nRecall: 0.00%\nF beta: 0.00\nChi2 stat (McNemar's test): 9.00\nDegrees of Freedom: 1\np-value: 0.00\n\n\nHere, we show the otu barcodes who most contributed to the best model’s predictions. A cut off of \\(0.1\\) is used to filter the most significant OTUs/features.\n\nbest_model = search.best_estimator_\nfeature_importances = best_model.feature_importances_\nimportant_indices = np.where(feature_importances &gt; 0.01)[0]\n\nfiltered_feature_importance = pd.DataFrame({\n    \"features\": rarefiedData.columns[important_indices],\n    \"importance\": feature_importances[important_indices]\n})\n\nplt.barh(filtered_feature_importance.features, filtered_feature_importance.importance)\nplt.xlabel('Feature Importance')\nplt.title('Feature Importance in Random Forest Classifier (above 1% importance)')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism concern groups</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_pca_and_grid_search.html",
    "href": "asd_concern_random_forest_with_pca_and_grid_search.html",
    "title": "3  Random-Forest analysis of autism concern groups, features reduced with PCA",
    "section": "",
    "text": "3.1 Preface\nThis document is written using Quarto, a modern scientific documentation tool by Postix. This tool functions like Rmarkdown or Jupyter and actually uses those tools to run the documents. It provides flexability in rendering documents and intigrates well with Posit’s other product, R Studio.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random-Forest analysis of autism concern groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_pca_and_grid_search.html#imports",
    "href": "asd_concern_random_forest_with_pca_and_grid_search.html#imports",
    "title": "3  Random-Forest analysis of autism concern groups, features reduced with PCA",
    "section": "3.2 Imports",
    "text": "3.2 Imports\n\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport yaml\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import \\\n     accuracy_score,\\\n     confusion_matrix,\\\n     f1_score, \\\n     make_scorer, \\\n     precision_recall_fscore_support\nfrom sklearn.model_selection import \\\n     GridSearchCV, \\\n     StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import chi2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random-Forest analysis of autism concern groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_pca_and_grid_search.html#get-data",
    "href": "asd_concern_random_forest_with_pca_and_grid_search.html#get-data",
    "title": "3  Random-Forest analysis of autism concern groups, features reduced with PCA",
    "section": "3.3 Get Data",
    "text": "3.3 Get Data\n\n3.3.1 Load Config\nLoading data into env\n\nwith open('config.yaml', mode='r', encoding=None) as f:\n    config_data = yaml.safe_load(f)\n\nfigure_export = config_data[\"figure_export\"] # default true\ndata_sources = config_data[\"data\"]\nparam_grid = config_data[\"param_grid\"]\nhyperparameters = config_data[\"hyperparameters\"]\n\nYaml config data\n\n\nfigure_export: true\ndata:\n  rarefied_table: \"data/rarefied_table.txt\"\n  mapper: \"data/mapper_book_copy_with_new_meta_titles copy.xlsx\"\nhyperparameters:\n  random_state: 42\n  training_split: 0.2\n  estimators: 100\n  n_jobs: -1\n  n_components: 0.60\n\nparam_grid:\n    max_features:\n      - ~\n      - sqrt\n      - log2\n    criterion: \n      - gini\n      - entropy\n      - log_loss\n    'class_weight':\n      - ~\n      - balanced\n\n\n\n\n\n3.3.2 Hyperparameters\nThese hyperparameters are established in the config.yaml file but can be overriden using command line values using -P variable:value -P othervariable:value syntax.\n\n# above tag allows parameters to be passed via cli\nestimators = hyperparameters[\"estimators\"]\nn_components = hyperparameters[\"n_components\"]\nn_jobs = hyperparameters[\"n_jobs\"]\nrandomState = hyperparameters[\"random_state\"]\ntrainingSplit = hyperparameters[\"training_split\"]\n\n\n\n3.3.3 Load dataframes\n\nrarefiedData = pd.read_csv(data_sources[\"rarefied_table\"], sep=\"\\t\")\n\nmapper = pd.read_excel(data_sources[\"mapper\"], sheet_name=\"Mapper 2025-03-21\")\n\nrarefiedData = rarefiedData.T\nrarefiedData.columns = rarefiedData.iloc[0].values\nrarefiedData = rarefiedData.iloc[1:]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random-Forest analysis of autism concern groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_pca_and_grid_search.html#create-training-and-testing-sets",
    "href": "asd_concern_random_forest_with_pca_and_grid_search.html#create-training-and-testing-sets",
    "title": "3  Random-Forest analysis of autism concern groups, features reduced with PCA",
    "section": "3.4 Create training and testing sets",
    "text": "3.4 Create training and testing sets\n\n3.4.1 Combining rarefied and asd status\n\nadosClassification = mapper.loc[:, [\"IDnumber\", \"@SampleID\", \"ADOSClassification\"]]\n\n# Filters out any potential stray IDs not part of out two groups\nadosClassification = adosClassification[adosClassification[\"IDnumber\"].str.contains('^[777|888]')]\n\nautismClassification = adosClassification\nautismClassification[\"ADOSClassification\"] = \\\n    adosClassification[\"ADOSClassification\"].astype(str)\nautismClassification = \\\n    autismClassification[autismClassification[\"ADOSClassification\"] != \"nan\"]\n\nautismClassification[\"Autism\"] = \\\n    autismClassification[\"ADOSClassification\"].transform(\n        lambda x: 0 if x == \"Minimal to No concern\" else 1\n        )\nautismClassification = autismClassification.drop(\n    columns = [\"ADOSClassification\"]\n    )\n\nadosRarefiedData = pd.merge(\n    autismClassification, \n    rarefiedData, \n    left_on = \"@SampleID\", \n    right_index = True, how=\"inner\")\nadosRarefiedData\n\n/var/folders/0w/00j3lxtn6lq5n0jfhbsvrdd00000gn/T/ipykernel_80329/3272697402.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  autismClassification[\"Autism\"] = \\\n\n\n\n\n\n\n\n\n\nIDnumber\n@SampleID\nAutism\n5fa6f990436cc98852c849cfc171976d\n9fc95dae73ec157e4f5ade5e0b205d51\nbf7ae65173c37bc3b210305f65760434\n523f0825433a02f5156e465f0972bc02\ncf87b63430ff2925721ab988ad584f83\n3c142dbbe6d67603d586ecb204a34cee\nec24451f7df9ad0d4edc00d327ba5b95\n...\na3b79adce4a953d4c1c8777dafc43a64\n86337e85ead9f35be4922027ed204e47\n4bfc0f32efe482acd73c942b8d7cd588\n35f42945e50c96278af7e268f124d56d\nd7656aada2e1e434542c5b7ed5592105\nd7bee6730fc4894b26aee43930e74710\n21d97d92de8f9fda7cc194d3245f039b\n304df219bf9ecbefbec639606e7bb98a\nf75aac2353d9e8da5f200c15cf7a564e\ne8dfa53470afb7592c0a10ecd09f758f\n\n\n\n\n0\n777-001\nasd965\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n777-001\nasd966\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.2\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n777-001\nasd1132\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n777-001\nasd1133\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.8\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n777-001\nasd1534\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n287\n888-029\nasd1407\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n288\n888-029\nasd1134\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n289\n888-029\nasd1135\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n290\n888-029\nasd1498\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n291\n888-029\nasd1499\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.2\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n184 rows × 847 columns\n\n\n\n\n\n3.4.2 Dividing training and testing\nThe following cells divide the data into training and testing sets. In order to keep the data clean, samples collected from a single individual, the IDnumber column, are divided togeather to avoid data bleed.\n\n# filter IDnumber and Autism \n# Y = autism, X = IDnumber\n# filter whole table on IDnumber\n# select real X values\ntemp_data = adosRarefiedData.loc[:,[\"IDnumber\", \"Autism\"]]\ntemp_data = temp_data.drop_duplicates()\nX_temp = temp_data.loc[:, \"IDnumber\"]\ny_temp = temp_data.loc[:, \"Autism\"]\n\nX_train_temp, X_test_temp, y_train_temp, y_test_temp = \\\n    train_test_split(X_temp, y_temp, test_size=trainingSplit,random_state=randomState)\n\n\ntrain_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_train_temp)]\ntest_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_test_temp)]\n\nnot_in_y = [\"Autism\", \"IDnumber\", \"@SampleID\"]\n\ny_train = train_data[\"Autism\"].to_frame()\nX_train = train_data.drop(columns = not_in_y)\ny_test = test_data[\"Autism\"].to_frame()\nX_test = test_data.drop(columns = not_in_y)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random-Forest analysis of autism concern groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_pca_and_grid_search.html#training",
    "href": "asd_concern_random_forest_with_pca_and_grid_search.html#training",
    "title": "3  Random-Forest analysis of autism concern groups, features reduced with PCA",
    "section": "3.5 Training",
    "text": "3.5 Training\nThe scaler rescales the features to fit between -1 and 1. This does not effect the outcome of the random forest but can help standardize inputs to increase efficiancy of training.\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_test = y_test.to_numpy().ravel()\ny_train = y_train.to_numpy().ravel()\n\nPCA for dimentionality reduction\n\npca = PCA(n_components=n_components)\nX_train_pca = pca.fit_transform(X_train)\nX_train_pca = pd.DataFrame(X_train_pca)\n\nGridSearchCV allows to train multiple Random Forest classifiers using various combinations of hyperparameters. The number of models trained is equal to \\((\\prod_{i=0}^{n}{|h_{i}|})*cv\\) where \\(|h_{i}|\\) is the size of the param param_grid list and \\(cv\\) is the cross validation number. StratifiedKFold divides the training data into n “folds”. Each pass of training selects one of these folds to act as a validation set to track the model’s training. More information can be found here. 5 is an arbitrary value.\n\nclassifier = RandomForestClassifier(n_estimators=estimators, random_state=randomState)\ncustom_f1 = make_scorer(f1_score, pos_label=1)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=randomState)\n\nsearch = GridSearchCV(\n    estimator=classifier,\n    n_jobs=n_jobs,\n    param_grid=param_grid,\n    cv=cv,\n    scoring=\"recall\")\nsearch.fit(X_train_pca, y_train)\nsearch.best_estimator_.get_params()\n\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': 'balanced',\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': 42,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\n3.5.1 Prediction\n\nX_test_pca = pca.transform(X_test)\ny_pred = search.predict(X_test_pca)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random-Forest analysis of autism concern groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_pca_and_grid_search.html#evaluations",
    "href": "asd_concern_random_forest_with_pca_and_grid_search.html#evaluations",
    "title": "3  Random-Forest analysis of autism concern groups, features reduced with PCA",
    "section": "3.6 Evaluations",
    "text": "3.6 Evaluations\nPrecision and recall are inversly preportional. Both values are combined in the F_beta score. F_beta is the harmonic mean of precision and recall scaled using the \\(\\beta\\) value. \\(\\beta\\) determines if precision or recall is weighted more in the final score. \\(\\beta = 1\\) equally balances both metrics; \\(\\beta &gt; 1\\) increases recalls weight and \\(\\beta &lt; 1\\) increases precisions weight. In this case, \\(\\beta\\) is set to 1.\n\nX_train_pca.columns = [f\"PC{x}\" for x in X_train_pca.columns]\n\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprecision, recall, fscore, _ = \\\n    precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\n\ntickLabels = [\"Mild-Severe Concern\", \"No Concern\"]\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False,\n            xticklabels=tickLabels, yticklabels=tickLabels)\n\nplt.title('Autism Predictions Confusion Matrix Heatmap')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n# McNemar's test\nchi2_stat = (conf_matrix[0,1] - conf_matrix[1,0])**2 / (conf_matrix[0,1] + conf_matrix[1,0])\np_val = chi2.sf(chi2_stat, df=1)\n\nprint(\n    f\"Accuracy: {accuracy * 100:.2f}%\\n\" \\\n    f\"Precision: {precision* 100:.2f}%\\n\" \\\n    f\"Recall: {recall* 100:.2f}%\\n\" \\\n    f\"F beta: {fscore:.2f}\\n\" \\\n    f\"Chi2 stat (McNemar's test): {chi2_stat:.2f}\\n\" \\\n    f\"Degrees of Freedom: {1}\\n\" \\\n    f\"p-value: {p_val:.2f}\"\n    )\n\n\n\n\n\n\n\n\nAccuracy: 68.42%\nPrecision: 20.00%\nRecall: 11.11%\nF beta: 0.14\nChi2 stat (McNemar's test): 1.33\nDegrees of Freedom: 1\np-value: 0.25\n\n\nHere, we show the otu barcodes who most contributed to the best model’s predictions. A cut off of \\(0.1\\) is used to filter the most significant OTUs/features.\n\nbest_model = search.best_estimator_\nprinciple_components = best_model.feature_importances_\nimportant_indices = np.where(principle_components &gt; 0.015)[0]\n\nfiltered_principle_components = pd.DataFrame({\n    \"components\": X_train_pca.columns[important_indices],\n    \"importance\": principle_components[important_indices]\n})\n\nplt.barh(filtered_principle_components.components, filtered_principle_components.importance)\nplt.axvline(x=0.015, color='r', linestyle='-', label='Critical Value')\nplt.xlabel('Feature Importance')\nplt.title('Feature Importance in Random Forest Classifier (above 1.5% importance)')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random-Forest analysis of autism concern groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_data_filtering_and_grid_search.html",
    "href": "asd_concern_random_forest_with_data_filtering_and_grid_search.html",
    "title": "4  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "",
    "text": "4.1 Preface\nThis document is written using Quarto, a modern scientific documentation tool by Postix. This tool functions like Rmarkdown or Jupyter and actually uses those tools to run the documents. It provides flexability in rendering documents and intigrates well with Posit’s other product, R Studio.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#imports",
    "href": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#imports",
    "title": "4  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "4.2 Imports",
    "text": "4.2 Imports\n\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport yaml\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import \\\n     accuracy_score,\\\n     confusion_matrix,\\\n     f1_score, \\\n     make_scorer, \\\n     precision_recall_fscore_support\nfrom sklearn.model_selection import \\\n     GridSearchCV, \\\n     StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import chi2",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#get-data",
    "href": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#get-data",
    "title": "4  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "4.3 Get Data",
    "text": "4.3 Get Data\n\n4.3.1 Load Config\nLoading data into env\n\nwith open('config.yaml', mode='r', encoding=None) as f:\n    config_data = yaml.safe_load(f)\n\nfigure_export = config_data[\"figure_export\"] # default true\ndata_sources = config_data[\"data\"]\nparam_grid = config_data[\"param_grid\"]\nhyperparameters = config_data[\"hyperparameters\"]\n\nYaml config data\n\n\nfigure_export: true\ndata:\n  rarefied_table: \"data/rarefied_table.txt\"\n  mapper: \"data/mapper_book_copy_with_new_meta_titles copy.xlsx\"\nhyperparameters:\n  random_state: 42\n  training_split: 0.2\n  estimators: 100\n  n_jobs: -1\n  n_components: 0.60\n\nparam_grid:\n    max_features:\n      - ~\n      - sqrt\n      - log2\n    criterion: \n      - gini\n      - entropy\n      - log_loss\n    'class_weight':\n      - ~\n      - balanced\n\n\n\n\n\n4.3.2 Hyperparameters\nThese hyperparameters are established in the config.yaml file but can be overriden using command line values using -P variable:value -P othervariable:value syntax.\n\n# above tag allows parameters to be passed via cli\nestimators = hyperparameters[\"estimators\"]\nn_components = hyperparameters[\"n_components\"]\nn_jobs = hyperparameters[\"n_jobs\"]\nrandomState = hyperparameters[\"random_state\"]\ntrainingSplit = hyperparameters[\"training_split\"]\n\n\n\n4.3.3 Load dataframes\n\nrarefiedData = pd.read_csv(data_sources[\"rarefied_table\"], sep=\"\\t\")\n\nmapper = pd.read_excel(data_sources[\"mapper\"], sheet_name=\"Mapper 2025-03-21\")\n\nrarefiedData = rarefiedData.T\nrarefiedData.columns = rarefiedData.iloc[0].values\nrarefiedData = rarefiedData.iloc[1:]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#create-training-and-testing-sets",
    "href": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#create-training-and-testing-sets",
    "title": "4  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "4.4 Create training and testing sets",
    "text": "4.4 Create training and testing sets\n\n4.4.1 Combining rarefied and asd status\nFiltering features that aren’t present in at least 20% of samples\n\nreads_frac_per_otu = rarefiedData.apply(lambda x: sum(x != 0)/len(x), axis=0)\nreads_frac_per_otu.name = \"frac\"\nreads_frac_per_otu = reads_frac_per_otu[reads_frac_per_otu &gt;= 0.2]\n# reads_frac_per_otu = pd.merge(reads_frac_per_otu, data, left_index=True, right_index=True).iloc[:,:2]\nfiltered_otus = rarefiedData[reads_frac_per_otu.index]\nfiltered_otus\n\n\n\n\n\n\n\n\nec24451f7df9ad0d4edc00d327ba5b95\ne6ca0ab8a0fd77337495add040a424b1\n29e46f338ff994729661c0c36dcb9952\n295b32a51421d1758d6bab8ecea6dd18\nae3d0d004da3a7ca7ecbe69fb52084a1\na7e9c4d158f9305597f3297c658e7f63\ndd658eb3e9a1f20b35812c4b9eebefc5\n18034589f32b77d60fce94836eda6e7e\n63b779c3cd79ac43fa4754ffee022f37\n4551087c7333dad3afc7bb73fda68a16\n...\nb1490ce4fd364d812f89c929941636e2\n77c6e4c581ab64246fd874fd4a032e96\n67e9678fbfd7b0f356cc3752d9018107\n01a39f312d4e84cef622982a2f50caee\nba3bd91a301ad07fadcd548998158ee0\nebe2bfb54ac2de1a404c6411b84abaa7\n160a294a1d67863cc2478274e2c109dd\n61f32bb71bb64e24c18e4fb16d193ace\n99eadd313db591f693de8906fa27c861\n637ab58cbef03d24007dd234008954d4\n\n\n\n\nasd1001\n0.0\n0.0\n0.0\n48.7\n5.9\n0.0\n0.0\n78.7\n3.5\n0.0\n...\n0.0\n1.2\n0.0\n5.7\n50.9\n0.0\n23.6\n0.0\n0.0\n0.0\n\n\nasd1002\n0.0\n0.0\n0.0\n603.7\n422.7\n128.5\n0.0\n0.0\n259.6\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.4\n0.0\n\n\nasd1004\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n37.9\n0.0\n2.1\n0.0\n\n\nasd1005\n0.0\n2.8\n21.5\n8.4\n9.0\n0.0\n0.0\n0.0\n0.0\n11.0\n...\n0.0\n0.0\n0.0\n1.3\n0.0\n0.0\n45.9\n454.2\n0.0\n0.0\n\n\nasd1006\n0.0\n0.0\n59.4\n0.0\n15.2\n0.0\n0.0\n38.3\n51.7\n10.0\n...\n0.0\n0.0\n9.5\n0.0\n0.0\n0.0\n124.3\n0.0\n0.0\n8.3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nasd995\n0.0\n0.0\n0.0\n0.0\n686.8\n0.0\n0.0\n8.8\n0.0\n0.0\n...\n28.7\n0.0\n12.2\n0.0\n0.0\n0.0\n0.0\n0.0\n212.0\n0.0\n\n\nasd996\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n35.5\n0.0\n29.6\n0.0\n0.0\n0.0\n0.0\n0.0\n3.2\n0.0\n\n\nasd997\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.2\n0.0\n0.0\n...\n118.7\n0.0\n24.8\n0.0\n0.0\n0.0\n0.0\n0.0\n17.9\n0.0\n\n\nasd998\n0.0\n0.0\n0.0\n5.4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nasd999\n0.0\n0.0\n0.0\n27.7\n0.0\n0.0\n0.0\n5.1\n0.7\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n272 rows × 92 columns\n\n\n\n\nadosClassification = mapper.loc[:, [\"IDnumber\", \"@SampleID\", \"ADOSClassification\"]]\n\n# Filters out any potential stray IDs not part of out two groups\nadosClassification = adosClassification[adosClassification[\"IDnumber\"].str.contains('^[777|888]')]\n\nautismClassification = adosClassification\nautismClassification[\"ADOSClassification\"] = \\\n    adosClassification[\"ADOSClassification\"].astype(str)\nautismClassification = \\\n    autismClassification[autismClassification[\"ADOSClassification\"] != \"nan\"]\n\nautismClassification[\"Autism\"] = \\\n    autismClassification[\"ADOSClassification\"].transform(\n        lambda x: 0 if x == \"Minimal to No concern\" else 1\n        )\nautismClassification = autismClassification.drop(\n    columns = [\"ADOSClassification\"]\n    )\n\nadosRarefiedData = pd.merge(\n    autismClassification, \n    filtered_otus, \n    left_on = \"@SampleID\", \n    right_index = True, how=\"inner\")\nadosRarefiedData\n\n/var/folders/0w/00j3lxtn6lq5n0jfhbsvrdd00000gn/T/ipykernel_79731/630839910.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  autismClassification[\"Autism\"] = \\\n\n\n\n\n\n\n\n\n\nIDnumber\n@SampleID\nAutism\nec24451f7df9ad0d4edc00d327ba5b95\ne6ca0ab8a0fd77337495add040a424b1\n29e46f338ff994729661c0c36dcb9952\n295b32a51421d1758d6bab8ecea6dd18\nae3d0d004da3a7ca7ecbe69fb52084a1\na7e9c4d158f9305597f3297c658e7f63\ndd658eb3e9a1f20b35812c4b9eebefc5\n...\nb1490ce4fd364d812f89c929941636e2\n77c6e4c581ab64246fd874fd4a032e96\n67e9678fbfd7b0f356cc3752d9018107\n01a39f312d4e84cef622982a2f50caee\nba3bd91a301ad07fadcd548998158ee0\nebe2bfb54ac2de1a404c6411b84abaa7\n160a294a1d67863cc2478274e2c109dd\n61f32bb71bb64e24c18e4fb16d193ace\n99eadd313db591f693de8906fa27c861\n637ab58cbef03d24007dd234008954d4\n\n\n\n\n0\n777-001\nasd965\n0\n0.0\n0.0\n0.0\n278.0\n6.0\n0.0\n3.2\n...\n0.0\n0.0\n0.0\n43.9\n0.0\n0.0\n90.6\n0.0\n0.0\n0.0\n\n\n1\n777-001\nasd966\n0\n6.2\n0.0\n14.4\n289.2\n17.6\n0.0\n3.9\n...\n0.0\n0.0\n0.0\n89.0\n30.5\n0.0\n142.1\n0.0\n0.0\n0.0\n\n\n2\n777-001\nasd1132\n0\n0.0\n0.0\n16.2\n201.4\n64.2\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n60.5\n33.8\n14.3\n36.3\n7.1\n0.0\n0.0\n\n\n3\n777-001\nasd1133\n0\n10.8\n0.0\n81.6\n352.5\n9.7\n0.0\n6.1\n...\n0.0\n0.0\n0.0\n190.0\n38.6\n86.9\n110.0\n9.8\n0.0\n0.0\n\n\n4\n777-001\nasd1534\n0\n0.0\n0.0\n69.2\n244.4\n26.6\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n164.5\n0.0\n63.8\n103.6\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n287\n888-029\nasd1407\n0\n0.0\n0.0\n0.0\n541.5\n0.0\n0.0\n3.9\n...\n0.0\n0.0\n0.0\n22.2\n0.0\n0.0\n144.1\n0.0\n0.0\n13.3\n\n\n288\n888-029\nasd1134\n0\n0.0\n0.0\n0.0\n27.7\n89.7\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n18.2\n67.4\n0.0\n59.4\n0.0\n0.0\n5.3\n\n\n289\n888-029\nasd1135\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.3\n0.0\n0.0\n0.0\n0.0\n0.0\n2.3\n0.0\n45.9\n0.0\n\n\n290\n888-029\nasd1498\n0\n2.0\n0.0\n7.9\n168.8\n0.0\n0.0\n0.0\n...\n0.0\n1.1\n0.0\n2.1\n0.0\n5.1\n214.2\n12.8\n0.0\n12.7\n\n\n291\n888-029\nasd1499\n0\n3.2\n0.0\n0.0\n248.4\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n5.7\n29.4\n5.0\n156.8\n0.0\n0.0\n10.3\n\n\n\n\n184 rows × 95 columns\n\n\n\n\n\n4.4.2 Dividing training and testing\nThe following cells divide the data into training and testing sets. In order to keep the data clean, samples collected from a single individual, the IDnumber column, are divided togeather to avoid data bleed.\n\n# filter IDnumber and Autism \n# Y = autism, X = IDnumber\n# filter whole table on IDnumber\n# select real X values\ntemp_data = adosRarefiedData.loc[:,[\"IDnumber\", \"Autism\"]]\ntemp_data = temp_data.drop_duplicates()\nX_temp = temp_data.loc[:, \"IDnumber\"]\ny_temp = temp_data.loc[:, \"Autism\"]\n\nX_train_temp, X_test_temp, y_train_temp, y_test_temp = \\\n    train_test_split(X_temp, y_temp, test_size=trainingSplit,random_state=randomState)\n\n\ntrain_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_train_temp)]\ntest_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_test_temp)]\n\nnot_in_y = [\"Autism\", \"IDnumber\", \"@SampleID\"]\n\ny_train = train_data[\"Autism\"].to_frame()\nX_train = train_data.drop(columns = not_in_y)\ny_test = test_data[\"Autism\"].to_frame()\nX_test = test_data.drop(columns = not_in_y)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#training",
    "href": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#training",
    "title": "4  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "4.5 Training",
    "text": "4.5 Training\nThe scaler rescales the features to fit between -1 and 1. This does not effect the outcome of the random forest but can help standardize inputs to increase efficiancy of training.\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_test = y_test.to_numpy().ravel()\ny_train = y_train.to_numpy().ravel()\n\nGridSearchCV allows to train multiple Random Forest classifiers using various combinations of hyperparameters. The number of models trained is equal to \\((\\prod_{i=0}^{n}{|h_{i}|})*cv\\) where \\(|h_{i}|\\) is the size of the param param_grid list and \\(cv\\) is the cross validation number. StratifiedKFold divides the training data into n “folds”. Each pass of training selects one of these folds to act as a validation set to track the model’s training. More information can be found here. 5 is an arbitrary value.\n\nclassifier = RandomForestClassifier(n_estimators=estimators, random_state=randomState)\ncustom_f1 = make_scorer(f1_score, pos_label=1)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=randomState)\n\nsearch = GridSearchCV(\n    estimator=classifier,\n    n_jobs=n_jobs,\n    param_grid=param_grid,\n    cv=cv,\n    scoring=\"recall\")\nsearch.fit(X_train, y_train)\nsearch.best_estimator_.get_params()\n\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': 42,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\n4.5.1 Prediction\n\ny_pred = search.predict(X_test)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#evaluations",
    "href": "asd_concern_random_forest_with_data_filtering_and_grid_search.html#evaluations",
    "title": "4  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "4.6 Evaluations",
    "text": "4.6 Evaluations\nPrecision and recall are inversly preportional. Both values are combined in the F_beta score. F_beta is the harmonic mean of precision and recall scaled using the \\(\\beta\\) value. \\(\\beta\\) determines if precision or recall is weighted more in the final score. \\(\\beta = 1\\) equally balances both metrics; \\(\\beta &gt; 1\\) increases recalls weight and \\(\\beta &lt; 1\\) increases precisions weight. In this case, \\(\\beta\\) is set to 1.\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprecision, recall, fscore, _ = \\\n    precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\n\ntickLabels = [\"Mild-Severe Concern\", \"No Concern\"]\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False,\n            xticklabels=tickLabels, yticklabels=tickLabels)\n\nplt.title('Autism Predictions Confusion Matrix Heatmap')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n# McNemar's test\nchi2_stat = (conf_matrix[0,1] - conf_matrix[1,0])**2 / (conf_matrix[0,1] + conf_matrix[1,0])\np_val = chi2.sf(chi2_stat, df=1)\n\nprint(\n    f\"Accuracy: {accuracy * 100:.2f}%\\n\" \\\n    f\"Precision: {precision* 100:.2f}%\\n\" \\\n    f\"Recall: {recall* 100:.2f}%\\n\" \\\n    f\"F beta: {fscore:.2f}\\n\" \\\n    f\"Chi2 stat (McNemar's test): {chi2_stat:.2f}\\n\" \\\n    f\"Degrees of Freedom: {1}\\n\" \\\n    f\"p-value: {p_val:.2f}\"\n    )\n\n\n\n\n\n\n\n\nAccuracy: 81.58%\nPrecision: 100.00%\nRecall: 22.22%\nF beta: 0.36\nChi2 stat (McNemar's test): 7.00\nDegrees of Freedom: 1\np-value: 0.01\n\n\nHere, we show the otu barcodes who most contributed to the best model’s predictions. A cut off of \\(0.1\\) is used to filter the most significant OTUs/features.\n\nbest_model = search.best_estimator_\nfeature_importances = best_model.feature_importances_\nimportant_indices = np.where(feature_importances &gt; 0.01)[0]\n\nfiltered_feature_importance = pd.DataFrame({\n    \"features\": rarefiedData.columns[important_indices],\n    \"importance\": feature_importances[important_indices]\n})\n\nplt.barh(filtered_feature_importance.features, filtered_feature_importance.importance)\nplt.xlabel('Feature Importance')\nplt.title('Feature Importance in Random Forest Classifier (above 1% importance)')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_grid_search.html",
    "href": "asd_likelihood_random_forest_with_grid_search.html",
    "title": "5  Basic Random-Forest analysis of autism likelihood groups",
    "section": "",
    "text": "5.1 Imports\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport yaml\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import\\\n     accuracy_score,\\\n     confusion_matrix,\\\n     f1_score, \\\n     make_scorer, \\\n     precision_recall_fscore_support\nfrom sklearn.model_selection import \\\n     GridSearchCV, \\\n     StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import chi2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism likelihood groups</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_grid_search.html#get-data",
    "href": "asd_likelihood_random_forest_with_grid_search.html#get-data",
    "title": "5  Basic Random-Forest analysis of autism likelihood groups",
    "section": "5.2 Get Data",
    "text": "5.2 Get Data\n\n5.2.1 Load Config\nLoading data into env\n\nwith open('config.yaml', mode='r', encoding=None) as f:\n    config_data = yaml.safe_load(f)\n\nfigure_export = config_data[\"figure_export\"] # default true\ndata_sources = config_data[\"data\"]\nparam_grid = config_data[\"param_grid\"]\nhyperparameters = config_data[\"hyperparameters\"]\n\n\n\n5.2.2 Hyperparameters\nThese hyperparameters are established in the config.yaml file but can be overriden using command line values using -P variable:value -P othervariable:value syntax.\n\n# above tag allows parameters to be passed via cli\nrandomState = hyperparameters[\"random_state\"]\ntrainingSplit = hyperparameters[\"training_split\"]\nestimators = hyperparameters[\"estimators\"]\nn_jobs = hyperparameters[\"n_jobs\"]\n\n\n\n5.2.3 Load dataframes\n\nrarefiedData = pd.read_csv(data_sources[\"rarefied_table\"], sep=\"\\t\")\n\nmapper = pd.read_excel(data_sources[\"mapper\"], sheet_name=\"Mapper 2025-03-21\")\n\nrarefiedData = rarefiedData.T\nrarefiedData.columns = rarefiedData.iloc[0].values\nrarefiedData = rarefiedData.iloc[1:]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism likelihood groups</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_grid_search.html#create-training-and-testing-sets",
    "href": "asd_likelihood_random_forest_with_grid_search.html#create-training-and-testing-sets",
    "title": "5  Basic Random-Forest analysis of autism likelihood groups",
    "section": "5.3 Create training and testing sets",
    "text": "5.3 Create training and testing sets\n\n5.3.1 Combining rarefied and asd status\n\nadosLikelihood = mapper.loc[:, [\"IDnumber\", \"@SampleID\"]]\n\n# Filters out any potential stray IDs not part of out two groups\nadosLikelihood = adosLikelihood[adosLikelihood[\"IDnumber\"].str.contains('^[777|888]')]\n\nadosLikelihood[\"Likelihood_Group\"] = adosLikelihood[\"IDnumber\"].transform(\n        lambda x: \"Elivated\" if x.startswith(\"777\") else \"Standard\"\n        )\n\nadosRarefiedData = pd.merge(\n    adosLikelihood, \n    rarefiedData, \n    left_on = \"@SampleID\", \n    right_index = True, how=\"inner\")\nadosRarefiedData\n\n\n\n\n\n\n\n\nIDnumber\n@SampleID\nLikelihood_Group\n5fa6f990436cc98852c849cfc171976d\n9fc95dae73ec157e4f5ade5e0b205d51\nbf7ae65173c37bc3b210305f65760434\n523f0825433a02f5156e465f0972bc02\ncf87b63430ff2925721ab988ad584f83\n3c142dbbe6d67603d586ecb204a34cee\nec24451f7df9ad0d4edc00d327ba5b95\n...\na3b79adce4a953d4c1c8777dafc43a64\n86337e85ead9f35be4922027ed204e47\n4bfc0f32efe482acd73c942b8d7cd588\n35f42945e50c96278af7e268f124d56d\nd7656aada2e1e434542c5b7ed5592105\nd7bee6730fc4894b26aee43930e74710\n21d97d92de8f9fda7cc194d3245f039b\n304df219bf9ecbefbec639606e7bb98a\nf75aac2353d9e8da5f200c15cf7a564e\ne8dfa53470afb7592c0a10ecd09f758f\n\n\n\n\n0\n777-001\nasd965\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n777-001\nasd966\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.2\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n777-001\nasd1132\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n777-001\nasd1133\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.8\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n777-001\nasd1534\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n290\n888-029\nasd1498\nStandard\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n291\n888-029\nasd1499\nStandard\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.2\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n293\n888-030\nasd1115\nStandard\n0.0\n78.7\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n294\n888-030\nasd1136\nStandard\n0.0\n142.6\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n295\n888-030\nasd1137\nStandard\n0.0\n109.2\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n272 rows × 847 columns\n\n\n\n\n\n5.3.2 Dividing training and testing\nThe following cells divide the data into training and testing sets. In order to keep the data clean, samples collected from a single individual, the IDnumber column, are divided togeather to avoid data bleed.\n\n# filter IDnumber and Autism \n# Y = autism, X = IDnumber\n# filter whole table on IDnumber\n# select real X values\ntemp_data = adosRarefiedData.loc[:,[\"IDnumber\", \"Likelihood_Group\"]]\ntemp_data = temp_data.drop_duplicates()\nX_temp = temp_data.loc[:, \"IDnumber\"]\ny_temp = temp_data.loc[:, \"Likelihood_Group\"]\n\nX_train_temp, X_test_temp, y_train_temp, y_test_temp = \\\n    train_test_split(X_temp, y_temp, test_size=trainingSplit,random_state=randomState)\n\n\ntrain_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_train_temp)]\ntest_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_test_temp)]\n\nnot_in_y = [\"Likelihood_Group\", \"IDnumber\", \"@SampleID\"]\n\ny_train = train_data[\"Likelihood_Group\"].to_frame()\nX_train = train_data.drop(columns = not_in_y)\ny_test = test_data[\"Likelihood_Group\"].to_frame()\nX_test = test_data.drop(columns = not_in_y)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism likelihood groups</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_grid_search.html#training",
    "href": "asd_likelihood_random_forest_with_grid_search.html#training",
    "title": "5  Basic Random-Forest analysis of autism likelihood groups",
    "section": "5.4 Training",
    "text": "5.4 Training\nThe scaler rescales the features to fit between -1 and 1. This does not effect the outcome of the random forest but can help standardize inputs to increase efficiancy of training.\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_test = y_test.to_numpy().ravel()\ny_train = y_train.to_numpy().ravel()\n\nGridSearchCV allows to train multiple Random Forest classifiers using various combinations of hyperparameters. The number of models trained is equal to \\((\\prod_{i=0}^{n}{|h_{i}|})*cv\\) where \\(|h_{i}|\\) is the size of the param param_grid list and \\(cv\\) is the cross validation number. StratifiedKFold divides the training data into n “folds”. Each pass of training selects one of these folds to act as a validation set to track the model’s training. More information can be found here. 5 is an arbitrary value.\n\nclassifier = RandomForestClassifier(n_estimators=estimators, random_state=randomState)\ncustom_f1 = make_scorer(f1_score, pos_label='Elivated')\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=randomState)\n\nsearch = GridSearchCV(\n    estimator=classifier,\n    n_jobs=n_jobs,\n    param_grid=param_grid,\n    cv=cv,\n    scoring=custom_f1)\nsearch.fit(X_train, y_train)\nsearch.best_estimator_.get_params()\n\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'entropy',\n 'max_depth': None,\n 'max_features': 'log2',\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': 42,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\n5.4.1 Prediction\n\ny_pred = search.predict(X_test)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism likelihood groups</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_grid_search.html#evaluations",
    "href": "asd_likelihood_random_forest_with_grid_search.html#evaluations",
    "title": "5  Basic Random-Forest analysis of autism likelihood groups",
    "section": "5.5 Evaluations",
    "text": "5.5 Evaluations\nPrecision and recall are inversly preportional. Both values are combined in the F_beta score. F_beta is the harmonic mean of precision and recall scaled using the \\(\\beta\\) value. \\(\\beta\\) determines if precision or recall is weighted more in the final score. \\(\\beta = 1\\) equally balances both metrics; \\(\\beta &gt; 1\\) increases recalls weight and \\(\\beta &lt; 1\\) increases precisions weight. In this case, \\(\\beta\\) is set to 1.\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprecision, recall, fscore, _ = \\\n    precision_recall_fscore_support(y_test, y_pred, average='weighted')\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\ntickLabels = [\"Elevated\", \"Standard\"]\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False,\n            xticklabels=tickLabels, yticklabels=tickLabels)\n\nplt.title('Likelihood Predictions Confusion Matrix Heatmap')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# McNemar's test\nchi2_stat = (conf_matrix[0,1] - conf_matrix[1,0])**2 / (conf_matrix[0,1] + conf_matrix[1,0])\np_val = chi2.sf(chi2_stat, df=1)\n\nprint(\n    f\"Accuracy: {accuracy * 100:.2f}%\\n\" \\\n    f\"Precision: {precision* 100:.2f}%\\n\" \\\n    f\"Recall: {recall* 100:.2f}%\\n\" \\\n    f\"F beta: {fscore:.2f}\\n\" \\\n    f\"Chi2 stat (McNemar's test): {chi2_stat:.2f}\\n\" \\\n    f\"Degrees of Freedom: {1}\\n\" \\\n    f\"p-value: {p_val:.2f}\"\n    )\n\n\n\n\n\n\n\n\nAccuracy: 60.00%\nPrecision: 60.26%\nRecall: 60.00%\nF beta: 0.58\nChi2 stat (McNemar's test): 4.17\nDegrees of Freedom: 1\np-value: 0.04\n\n\nHere, we show the otu barcodes who most contributed to the best model’s predictions. A cut off of \\(0.1\\) is used to filter the most significant OTUs/features.\n\nbest_model = search.best_estimator_\nfeature_importances = best_model.feature_importances_\nimportant_indices = np.where(feature_importances &gt; 0.01)[0]\n\nfiltered_feature_importance = pd.DataFrame({\n    \"features\": rarefiedData.columns[important_indices],\n    \"importance\": feature_importances[important_indices]\n})\n\nplt.barh(filtered_feature_importance.features, filtered_feature_importance.importance)\nplt.xlabel('Feature Importance')\nplt.title('Likelihood analysis: Feature Importance (above 1% importance)')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Random-Forest analysis of autism likelihood groups</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_pca_and_grid_search.html",
    "href": "asd_likelihood_random_forest_with_pca_and_grid_search.html",
    "title": "6  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "",
    "text": "6.1 Preface\nThis document is written using Quarto, a modern scientific documentation tool by Postix. This tool functions like Rmarkdown or Jupyter and actually uses those tools to run the documents. It provides flexability in rendering documents and intigrates well with Posit’s other product, R Studio.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_pca_and_grid_search.html#imports",
    "href": "asd_likelihood_random_forest_with_pca_and_grid_search.html#imports",
    "title": "6  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "6.2 Imports",
    "text": "6.2 Imports\n\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport yaml\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import \\\n     accuracy_score,\\\n     confusion_matrix,\\\n     f1_score, \\\n     make_scorer, \\\n     precision_recall_fscore_support\nfrom sklearn.model_selection import \\\n     GridSearchCV, \\\n     StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import chi2",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_pca_and_grid_search.html#get-data",
    "href": "asd_likelihood_random_forest_with_pca_and_grid_search.html#get-data",
    "title": "6  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "6.3 Get Data",
    "text": "6.3 Get Data\n\n6.3.1 Load Config\nLoading data into env\n\nwith open('config.yaml', mode='r', encoding=None) as f:\n    config_data = yaml.safe_load(f)\n\nfigure_export = config_data[\"figure_export\"] # default true\ndata_sources = config_data[\"data\"]\nparam_grid = config_data[\"param_grid\"]\nhyperparameters = config_data[\"hyperparameters\"]\n\nYaml config data\n\n\nfigure_export: true\ndata:\n  rarefied_table: \"data/rarefied_table.txt\"\n  mapper: \"data/mapper_book_copy_with_new_meta_titles copy.xlsx\"\nhyperparameters:\n  random_state: 42\n  training_split: 0.2\n  estimators: 100\n  n_jobs: -1\n  n_components: 0.60\n\nparam_grid:\n    max_features:\n      - ~\n      - sqrt\n      - log2\n    criterion: \n      - gini\n      - entropy\n      - log_loss\n    'class_weight':\n      - ~\n      - balanced\n\n\n\n\n\n6.3.2 Hyperparameters\nThese hyperparameters are established in the config.yaml file but can be overriden using command line values using -P variable:value -P othervariable:value syntax.\n\n# above tag allows parameters to be passed via cli\nestimators = hyperparameters[\"estimators\"]\nn_components = hyperparameters[\"n_components\"]\nn_jobs = hyperparameters[\"n_jobs\"]\nrandomState = hyperparameters[\"random_state\"]\ntrainingSplit = hyperparameters[\"training_split\"]\n\n\n\n6.3.3 Load dataframes\n\nrarefiedData = pd.read_csv(data_sources[\"rarefied_table\"], sep=\"\\t\")\n\nmapper = pd.read_excel(data_sources[\"mapper\"], sheet_name=\"Mapper 2025-03-21\")\n\nrarefiedData = rarefiedData.T\nrarefiedData.columns = rarefiedData.iloc[0].values\nrarefiedData = rarefiedData.iloc[1:]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_pca_and_grid_search.html#create-training-and-testing-sets",
    "href": "asd_likelihood_random_forest_with_pca_and_grid_search.html#create-training-and-testing-sets",
    "title": "6  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "6.4 Create training and testing sets",
    "text": "6.4 Create training and testing sets\n\n6.4.1 Combining rarefied and asd status\n\nadosLikelihood = mapper.loc[:, [\"IDnumber\", \"@SampleID\"]]\n\n# Filters out any potential stray IDs not part of out two groups\nadosLikelihood = adosLikelihood[adosLikelihood[\"IDnumber\"].str.contains('^[777|888]')]\n\nadosLikelihood[\"Likelihood_Group\"] = adosLikelihood[\"IDnumber\"].transform(\n        lambda x: \"Elivated\" if x.startswith(\"777\") else \"Standard\"\n        )\n\nadosRarefiedData = pd.merge(\n    adosLikelihood, \n    rarefiedData, \n    left_on = \"@SampleID\", \n    right_index = True, how=\"inner\")\nadosRarefiedData\n\n\n\n\n\n\n\n\nIDnumber\n@SampleID\nLikelihood_Group\n5fa6f990436cc98852c849cfc171976d\n9fc95dae73ec157e4f5ade5e0b205d51\nbf7ae65173c37bc3b210305f65760434\n523f0825433a02f5156e465f0972bc02\ncf87b63430ff2925721ab988ad584f83\n3c142dbbe6d67603d586ecb204a34cee\nec24451f7df9ad0d4edc00d327ba5b95\n...\na3b79adce4a953d4c1c8777dafc43a64\n86337e85ead9f35be4922027ed204e47\n4bfc0f32efe482acd73c942b8d7cd588\n35f42945e50c96278af7e268f124d56d\nd7656aada2e1e434542c5b7ed5592105\nd7bee6730fc4894b26aee43930e74710\n21d97d92de8f9fda7cc194d3245f039b\n304df219bf9ecbefbec639606e7bb98a\nf75aac2353d9e8da5f200c15cf7a564e\ne8dfa53470afb7592c0a10ecd09f758f\n\n\n\n\n0\n777-001\nasd965\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n777-001\nasd966\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.2\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n777-001\nasd1132\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n777-001\nasd1133\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.8\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n777-001\nasd1534\nElivated\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n290\n888-029\nasd1498\nStandard\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n291\n888-029\nasd1499\nStandard\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.2\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n293\n888-030\nasd1115\nStandard\n0.0\n78.7\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n294\n888-030\nasd1136\nStandard\n0.0\n142.6\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n295\n888-030\nasd1137\nStandard\n0.0\n109.2\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n272 rows × 847 columns\n\n\n\n\n\n6.4.2 Dividing training and testing\nThe following cells divide the data into training and testing sets. In order to keep the data clean, samples collected from a single individual, the IDnumber column, are divided togeather to avoid data bleed.\n\n# filter IDnumber and Autism \n# Y = autism, X = IDnumber\n# filter whole table on IDnumber\n# select real X values\ntemp_data = adosRarefiedData.loc[:,[\"IDnumber\", \"Likelihood_Group\"]]\ntemp_data = temp_data.drop_duplicates()\nX_temp = temp_data.loc[:, \"IDnumber\"]\ny_temp = temp_data.loc[:, \"Likelihood_Group\"]\n\nX_train_temp, X_test_temp, y_train_temp, y_test_temp = \\\n    train_test_split(X_temp, y_temp, test_size=trainingSplit,random_state=randomState)\n\n\ntrain_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_train_temp)]\ntest_data = adosRarefiedData[adosRarefiedData[\"IDnumber\"].isin(X_test_temp)]\n\nnot_in_y = [\"Likelihood_Group\", \"IDnumber\", \"@SampleID\"]\n\ny_train = train_data[\"Likelihood_Group\"].to_frame()\nX_train = train_data.drop(columns = not_in_y)\ny_test = test_data[\"Likelihood_Group\"].to_frame()\nX_test = test_data.drop(columns = not_in_y)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_pca_and_grid_search.html#training",
    "href": "asd_likelihood_random_forest_with_pca_and_grid_search.html#training",
    "title": "6  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "6.5 Training",
    "text": "6.5 Training\nThe scaler rescales the features to fit between -1 and 1. This does not effect the outcome of the random forest but can help standardize inputs to increase efficiancy of training.\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ny_test = y_test.to_numpy().ravel()\ny_train = y_train.to_numpy().ravel()\n\nPCA for dimentionality reduction\n\npca = PCA(n_components=n_components)\nX_train_pca = pca.fit_transform(X_train)\nX_train_pca = pd.DataFrame(X_train_pca)\n\nGridSearchCV allows to train multiple Random Forest classifiers using various combinations of hyperparameters. The number of models trained is equal to \\((\\prod_{i=0}^{n}{|h_{i}|})*cv\\) where \\(|h_{i}|\\) is the size of the param param_grid list and \\(cv\\) is the cross validation number. StratifiedKFold divides the training data into n “folds”. Each pass of training selects one of these folds to act as a validation set to track the model’s training. More information can be found here. 5 is an arbitrary value.\n\nclassifier = RandomForestClassifier(n_estimators=estimators, random_state=randomState)\ncustom_f1 = make_scorer(f1_score, pos_label='Yes')\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=randomState)\n\nsearch = GridSearchCV(\n    estimator=classifier,\n    n_jobs=n_jobs,\n    param_grid=param_grid,\n    cv=cv,\n    scoring=custom_f1)\nsearch.fit(X_train_pca, y_train)\nsearch.best_estimator_.get_params()\n\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': 42,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\n6.5.1 Prediction\n\nX_test_pca = pca.transform(X_test)\ny_pred = search.predict(X_test_pca)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "asd_likelihood_random_forest_with_pca_and_grid_search.html#evaluations",
    "href": "asd_likelihood_random_forest_with_pca_and_grid_search.html#evaluations",
    "title": "6  Random-Forest analysis of autism likelihood groups, features reduced with PCA",
    "section": "6.6 Evaluations",
    "text": "6.6 Evaluations\nPrecision and recall are inversly preportional. Both values are combined in the F_beta score. F_beta is the harmonic mean of precision and recall scaled using the \\(\\beta\\) value. \\(\\beta\\) determines if precision or recall is weighted more in the final score. \\(\\beta = 1\\) equally balances both metrics; \\(\\beta &gt; 1\\) increases recalls weight and \\(\\beta &lt; 1\\) increases precisions weight. In this case, \\(\\beta\\) is set to 1.\n\nX_train_pca.columns = [f\"PC{x}\" for x in X_train_pca.columns]\n\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprecision, recall, fscore, _ = \\\n    precision_recall_fscore_support(y_test, y_pred, average='weighted')\n\n# Chi^2 Goodness of Fit\npred_counts = np.array([sum(y_pred == 'No'), sum(y_pred == 'Yes')])\nnull_counts = np.array([sum(y_test == 'No'), sum(y_test == 'Yes')])\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\ntickLabels = [\"No Concern\", \"Mild-Severe Concern\"]\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False,\n            xticklabels=tickLabels, yticklabels=tickLabels)\n\nplt.title('Autism Predictions Confusion Matrix Heatmap')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# McNemar's test\nchi2_stat = (conf_matrix[0,1] - conf_matrix[1,0])**2 / (conf_matrix[0,1] + conf_matrix[1,0])\np_val = chi2.sf(chi2_stat, df=1)\n\nprint(\n    f\"Accuracy: {accuracy * 100:.2f}%\\n\" \\\n    f\"Precision: {precision* 100:.2f}%\\n\" \\\n    f\"Recall: {recall* 100:.2f}%\\n\" \\\n    f\"F beta: {fscore:.2f}\\n\" \\\n    f\"Chi2 stat (McNemar's test): {chi2_stat:.2f}\\n\" \\\n    f\"Degrees of Freedom: {1}\\n\" \\\n    f\"p-value: {p_val:.2f}\"\n    )\n\n\n\n\n\n\n\n\nAccuracy: 46.67%\nPrecision: 46.41%\nRecall: 46.67%\nF beta: 0.46\nChi2 stat (McNemar's test): 0.12\nDegrees of Freedom: 1\np-value: 0.72\n\n\nHere, we show the otu barcodes who most contributed to the best model’s predictions. A cut off of \\(0.1\\) is used to filter the most significant OTUs/features.\n\nbest_model = search.best_estimator_\nprinciple_components = best_model.feature_importances_\nimportant_indices = np.where(principle_components &gt; 0.015)[0]\n\nfiltered_principle_components = pd.DataFrame({\n    \"components\": X_train_pca.columns[important_indices],\n    \"importance\": principle_components[important_indices]\n})\n\nplt.barh(filtered_principle_components.components, filtered_principle_components.importance)\nplt.axvline(x=0.015, color='r', linestyle='-', label='Critical Value')\nplt.xlabel('Feature Importance')\nplt.title('Feature Importance in Random Forest Classifier (above 1.5% importance)')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random-Forest analysis of autism likelihood groups, features reduced with PCA</span>"
    ]
  },
  {
    "objectID": "figures.html",
    "href": "figures.html",
    "title": "7  Figures",
    "section": "",
    "text": "7.1 Concern group predictions",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#concern-group-predictions",
    "href": "figures.html#concern-group-predictions",
    "title": "7  Figures",
    "section": "",
    "text": "Accuracy: 76.32%\nPrecision: 0.00%\nRecall: 0.00%\nF beta: 0.00\nChi2 stat (McNemar's test): 9.00\nDegrees of Freedom: 1\np-value: 0.00",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#concern-group-most-significant-otus",
    "href": "figures.html#concern-group-most-significant-otus",
    "title": "7  Figures",
    "section": "7.2 Concern group most significant OTUs",
    "text": "7.2 Concern group most significant OTUs",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#concern-group-pca-predictions",
    "href": "figures.html#concern-group-pca-predictions",
    "title": "7  Figures",
    "section": "7.3 Concern group PCA predictions",
    "text": "7.3 Concern group PCA predictions\n\n\n\n\n\n\n\n\n\n\nAccuracy: 68.42%\nPrecision: 20.00%\nRecall: 11.11%\nF beta: 0.14\nChi2 stat (McNemar's test): 1.33\nDegrees of Freedom: 1\np-value: 0.25",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#concern-group-pca-most-significant-otus",
    "href": "figures.html#concern-group-pca-most-significant-otus",
    "title": "7  Figures",
    "section": "7.4 Concern group PCA most significant OTUs",
    "text": "7.4 Concern group PCA most significant OTUs",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#concern-group-filtered-otu-predictions",
    "href": "figures.html#concern-group-filtered-otu-predictions",
    "title": "7  Figures",
    "section": "7.5 Concern group filtered OTU predictions",
    "text": "7.5 Concern group filtered OTU predictions\n\n\n\n\n\n\n\n\n\n\nAccuracy: 81.58%\nPrecision: 100.00%\nRecall: 22.22%\nF beta: 0.36\nChi2 stat (McNemar's test): 7.00\nDegrees of Freedom: 1\np-value: 0.01",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#concern-group-filtered-otu-most-significant-otus",
    "href": "figures.html#concern-group-filtered-otu-most-significant-otus",
    "title": "7  Figures",
    "section": "7.6 Concern group filtered OTU most significant OTUs",
    "text": "7.6 Concern group filtered OTU most significant OTUs",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#likelihood-group-predictions",
    "href": "figures.html#likelihood-group-predictions",
    "title": "7  Figures",
    "section": "7.7 Likelihood group predictions",
    "text": "7.7 Likelihood group predictions\n\n\n\n\n\n\n\n\n\n\nAccuracy: 60.00%\nPrecision: 60.26%\nRecall: 60.00%\nF beta: 0.58\nChi2 stat (McNemar's test): 4.17\nDegrees of Freedom: 1\np-value: 0.04",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#likelihood-group-most-significant-otus",
    "href": "figures.html#likelihood-group-most-significant-otus",
    "title": "7  Figures",
    "section": "7.8 Likelihood group most significant OTUs",
    "text": "7.8 Likelihood group most significant OTUs",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#likelihood-group-pca-predictions",
    "href": "figures.html#likelihood-group-pca-predictions",
    "title": "7  Figures",
    "section": "7.9 Likelihood group PCA predictions",
    "text": "7.9 Likelihood group PCA predictions\n\n\n\n\n\n\n\n\n\n\nAccuracy: 46.67%\nPrecision: 46.41%\nRecall: 46.67%\nF beta: 0.46\nChi2 stat (McNemar's test): 0.12\nDegrees of Freedom: 1\np-value: 0.72",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "figures.html#likelihood-group-pca-most-significant-otus",
    "href": "figures.html#likelihood-group-pca-most-significant-otus",
    "title": "7  Figures",
    "section": "7.10 Likelihood group PCA most significant OTUs",
    "text": "7.10 Likelihood group PCA most significant OTUs",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Figures</span>"
    ]
  }
]